{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:44.119702Z",
     "iopub.status.busy": "2025-08-16T07:29:44.119372Z",
     "iopub.status.idle": "2025-08-16T07:29:44.128655Z",
     "shell.execute_reply": "2025-08-16T07:29:44.127533Z",
     "shell.execute_reply.started": "2025-08-16T07:29:44.119665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing import Pipeline\n",
    "from darts.dataprocessing.transformers import Scaler, InvertibleMapper, StaticCovariatesTransformer\n",
    "from darts.dataprocessing.transformers.missing_values_filler import MissingValuesFiller\n",
    "from darts.metrics import rmsle\n",
    "from darts.models import LinearRegressionModel, LightGBMModel, XGBModel, CatBoostModel\n",
    "from darts.models.filtering.moving_average_filter import MovingAverageFilter\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import itertools\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams[\"font.size\"] = 15  \n",
    "COLORS = list(sns.color_palette())\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:44.130134Z",
     "iopub.status.busy": "2025-08-16T07:29:44.129786Z",
     "iopub.status.idle": "2025-08-16T07:29:44.151987Z",
     "shell.execute_reply": "2025-08-16T07:29:44.151051Z",
     "shell.execute_reply.started": "2025-08-16T07:29:44.130104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# helper function to print messages\n",
    "def cprint(title, *args):\n",
    "    print(\n",
    "        \"=\"*len(title), title, \"=\"*len(title),\n",
    "        *args,\n",
    "        sep=\"\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:44.153396Z",
     "iopub.status.busy": "2025-08-16T07:29:44.153054Z",
     "iopub.status.idle": "2025-08-16T07:29:46.299216Z",
     "shell.execute_reply": "2025-08-16T07:29:46.298309Z",
     "shell.execute_reply.started": "2025-08-16T07:29:44.153375Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\", parse_dates=[\"date\"]).drop(columns='onpromotion')\n",
    "test = pd.read_csv(\"test.csv\", parse_dates=[\"date\"]).drop(columns='onpromotion')\n",
    "\n",
    "oil = pd.read_csv('oil.csv', parse_dates=[\"date\"]).rename(columns={\"price\": \"oil\"})\n",
    "store = pd.read_csv(\"stores.csv\")\n",
    "holiday = pd.read_csv(\"holidays_events.csv\", parse_dates=[\"date\"])\n",
    "holiday_indonesia = pd.read_csv(\"Holiday Indonesian.csv\", parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208781</th>\n",
       "      <td>5208782</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>9</td>\n",
       "      <td>POULTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208782</th>\n",
       "      <td>5208783</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>9</td>\n",
       "      <td>PREPARED FOODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208783</th>\n",
       "      <td>5208784</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>9</td>\n",
       "      <td>PRODUCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208784</th>\n",
       "      <td>5208785</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>9</td>\n",
       "      <td>SCHOOL AND OFFICE SUPPLIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208785</th>\n",
       "      <td>5208786</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>9</td>\n",
       "      <td>SEAFOOD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5208786 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id       date  store_nbr                      family\n",
       "0              1 2017-08-16          1                  AUTOMOTIVE\n",
       "1              2 2017-08-16          1                   BABY CARE\n",
       "2              3 2017-08-16          1                      BEAUTY\n",
       "3              4 2017-08-16          1                   BEVERAGES\n",
       "4              5 2017-08-16          1                       BOOKS\n",
       "...          ...        ...        ...                         ...\n",
       "5208781  5208782 2025-08-16          9                     POULTRY\n",
       "5208782  5208783 2025-08-16          9              PREPARED FOODS\n",
       "5208783  5208784 2025-08-16          9                     PRODUCE\n",
       "5208784  5208785 2025-08-16          9  SCHOOL AND OFFICE SUPPLIES\n",
       "5208785  5208786 2025-08-16          9                     SEAFOOD\n",
       "\n",
       "[5208786 rows x 4 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_range = pd.date_range(start='2017-08-16', end='2025-08-16')\n",
    "stores = test['store_nbr'].unique()\n",
    "families = test['family'].unique()\n",
    "\n",
    "all_combinations = pd.DataFrame(\n",
    "    list(itertools.product(test_range, stores, families)),\n",
    "    columns=['date', 'store_nbr', 'family']\n",
    ")\n",
    "all_combinations['id'] = range(1, len(all_combinations)+1)\n",
    "test = all_combinations[['id', 'date', 'store_nbr', 'family']]\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:46.301716Z",
     "iopub.status.busy": "2025-08-16T07:29:46.301430Z",
     "iopub.status.idle": "2025-08-16T07:29:46.339201Z",
     "shell.execute_reply": "2025-08-16T07:29:46.338425Z",
     "shell.execute_reply.started": "2025-08-16T07:29:46.301696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>oil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>93.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>92.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>93.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>93.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>93.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>2025-08-11</td>\n",
       "      <td>63.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>2025-08-12</td>\n",
       "      <td>63.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>2025-08-13</td>\n",
       "      <td>62.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>2025-08-14</td>\n",
       "      <td>63.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>62.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3312 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date    oil\n",
       "0    2013-01-02  93.12\n",
       "1    2013-01-03  92.92\n",
       "2    2013-01-04  93.09\n",
       "3    2013-01-07  93.19\n",
       "4    2013-01-08  93.15\n",
       "...         ...    ...\n",
       "3307 2025-08-11  63.96\n",
       "3308 2025-08-12  63.17\n",
       "3309 2025-08-13  62.65\n",
       "3310 2025-08-14  63.96\n",
       "3311 2025-08-15  62.80\n",
       "\n",
       "[3312 rows x 2 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oil['date'] = pd.to_datetime(oil['date'], dayfirst=True)\n",
    "oil = oil.sort_values('date', ascending=True).reset_index().drop(columns=['index'])\n",
    "oil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check: There are missing gaps in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:46.550814Z",
     "iopub.status.busy": "2025-08-16T07:29:46.550537Z",
     "iopub.status.idle": "2025-08-16T07:29:47.085084Z",
     "shell.execute_reply": "2025-08-16T07:29:47.084103Z",
     "shell.execute_reply.started": "2025-08-16T07:29:46.550795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_family = train.family.nunique()\n",
    "num_store = train.store_nbr.nunique()\n",
    "num_ts = train.groupby([\"store_nbr\", \"family\"]).ngroups\n",
    "train_start = train.date.min().date()\n",
    "train_end = train.date.max().date()\n",
    "num_train_date = train.date.nunique()\n",
    "train_len = (train_end - train_start).days + 1\n",
    "test_start = test.date.min().date()\n",
    "test_end = test.date.max().date()\n",
    "num_test_date = test.date.nunique()\n",
    "test_len = (test_end - test_start).days + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:47.086263Z",
     "iopub.status.busy": "2025-08-16T07:29:47.086005Z",
     "iopub.status.idle": "2025-08-16T07:29:47.092047Z",
     "shell.execute_reply": "2025-08-16T07:29:47.090915Z",
     "shell.execute_reply.started": "2025-08-16T07:29:47.086237Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "Basic information of data\n",
      "=========================\n",
      "Number of family types      : 33\n",
      "Number of stores            : 54\n",
      "Number of store-family pairs: 1782\n",
      "Number of target series     : 1782\n",
      "\n",
      "Number of unique train dates: 1684\n",
      "Train date range            : 1688 days from 2013-01-01 to 2017-08-15\n",
      "Number of unique test dates : 2923\n",
      "Test date range             : 2923 days from 2017-08-16 to 2025-08-16\n"
     ]
    }
   ],
   "source": [
    "cprint(\n",
    "    \"Basic information of data\",\n",
    "    f\"Number of family types      : {num_family}\",\n",
    "    f\"Number of stores            : {num_store}\",\n",
    "    f\"Number of store-family pairs: {num_family * num_store}\",\n",
    "    f\"Number of target series     : {num_ts}\",\n",
    "    \"\",\n",
    "    f\"Number of unique train dates: {num_train_date}\",\n",
    "    f\"Train date range            : {train_len} days from {train_start} to {train_end}\",\n",
    "    f\"Number of unique test dates : {num_test_date}\",\n",
    "    f\"Test date range             : {test_len} days from {test_start} to {test_end}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check: The 4 missing dates fall on Christmas across the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:47.093526Z",
     "iopub.status.busy": "2025-08-16T07:29:47.093243Z",
     "iopub.status.idle": "2025-08-16T07:29:47.442965Z",
     "shell.execute_reply": "2025-08-16T07:29:47.442144Z",
     "shell.execute_reply.started": "2025-08-16T07:29:47.093504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "missing_dates = pd.date_range(train_start, train_end).difference(train.date.unique())\n",
    "missing_dates = missing_dates.strftime(\"%Y-%m-%d\").tolist()\n",
    "\n",
    "unique_dp_count = train.groupby([\"store_nbr\", \"family\"]).date.count().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:47.444413Z",
     "iopub.status.busy": "2025-08-16T07:29:47.444078Z",
     "iopub.status.idle": "2025-08-16T07:29:47.449574Z",
     "shell.execute_reply": "2025-08-16T07:29:47.448687Z",
     "shell.execute_reply.started": "2025-08-16T07:29:47.444383Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "Missing gaps in time series\n",
      "===========================\n",
      "List incl. unique counts of data points: [1684]\n",
      "Missing dates                          : ['2013-12-25', '2014-12-25', '2015-12-25', '2016-12-25']\n"
     ]
    }
   ],
   "source": [
    "cprint(\n",
    "    \"Missing gaps in time series\",\n",
    "    f\"List incl. unique counts of data points: {unique_dp_count}\",\n",
    "    f\"Missing dates                          : {missing_dates}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:47.451414Z",
     "iopub.status.busy": "2025-08-16T07:29:47.450651Z",
     "iopub.status.idle": "2025-08-16T07:29:48.876837Z",
     "shell.execute_reply": "2025-08-16T07:29:48.875942Z",
     "shell.execute_reply.started": "2025-08-16T07:29:47.451391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# reindex training data\n",
    "multi_idx = pd.MultiIndex.from_product(\n",
    "    [pd.date_range(train_start, train_end), train.store_nbr.unique(), train.family.unique()],\n",
    "    names=[\"date\", \"store_nbr\", \"family\"],\n",
    ")\n",
    "train = train.set_index([\"date\", \"store_nbr\", \"family\"]).reindex(multi_idx).reset_index()\n",
    "\n",
    "# fill missing values with 0s\n",
    "train[\"sales\"] = train[\"sales\"].fillna(0.)\n",
    "train.id = train.id.interpolate(method=\"linear\") # interpolate linearly as a filler for the 'id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check: There are no oil prices on weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:48.877959Z",
     "iopub.status.busy": "2025-08-16T07:29:48.877719Z",
     "iopub.status.idle": "2025-08-16T07:29:48.886329Z",
     "shell.execute_reply": "2025-08-16T07:29:48.885616Z",
     "shell.execute_reply.started": "2025-08-16T07:29:48.877940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "missing_oil_dates = pd.date_range(train_start, test_end).difference(oil.date)\n",
    "num_missing_oil_dates = len(missing_oil_dates)\n",
    "num_wknd_missing = (missing_oil_dates.weekday >= 5).sum()\n",
    "total_num_wknd = (pd.date_range(train_start, test_end).weekday >= 5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:48.887524Z",
     "iopub.status.busy": "2025-08-16T07:29:48.887212Z",
     "iopub.status.idle": "2025-08-16T07:29:48.902772Z",
     "shell.execute_reply": "2025-08-16T07:29:48.901861Z",
     "shell.execute_reply.started": "2025-08-16T07:29:48.887499Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "Missing oil dates\n",
      "=================\n",
      "Number of missing oil dates: 1299\n",
      "Number of weekends missing : 1273\n",
      "Total number of weekends   : 1317\n"
     ]
    }
   ],
   "source": [
    "cprint(\n",
    "    \"Missing oil dates\",\n",
    "    f\"Number of missing oil dates: {num_missing_oil_dates}\",\n",
    "    f\"Number of weekends missing : {num_wknd_missing}\",\n",
    "    f\"Total number of weekends   : {total_num_wknd}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:48.904261Z",
     "iopub.status.busy": "2025-08-16T07:29:48.903918Z",
     "iopub.status.idle": "2025-08-16T07:29:48.925244Z",
     "shell.execute_reply": "2025-08-16T07:29:48.924162Z",
     "shell.execute_reply.started": "2025-08-16T07:29:48.904234Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# reindex oil data\n",
    "oil = oil.merge(\n",
    "    pd.DataFrame({\"date\": pd.date_range(train_start, test_end)}),\n",
    "    on=\"date\",\n",
    "    how=\"outer\",\n",
    ").sort_values(\"date\", ignore_index=True)\n",
    "\n",
    "# fill missing values using linear interpolation\n",
    "oil.oil = oil.oil.interpolate(method=\"linear\", limit_direction=\"both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing On Holiday, the corresponden one is National only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:49.333589Z",
     "iopub.status.busy": "2025-08-16T07:29:49.333231Z",
     "iopub.status.idle": "2025-08-16T07:29:49.341857Z",
     "shell.execute_reply": "2025-08-16T07:29:49.340764Z",
     "shell.execute_reply.started": "2025-08-16T07:29:49.333558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "holiday['type'] = holiday['type'].replace({'Bridge': 'Additional', 'Event': 'Holiday'})\n",
    "holiday = holiday[~holiday['type'].isin(['Transfer', 'Work Day'])]\n",
    "holiday = holiday.reset_index(drop=True)\n",
    "holiday = pd.get_dummies(holiday, columns=['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:49.343539Z",
     "iopub.status.busy": "2025-08-16T07:29:49.343268Z",
     "iopub.status.idle": "2025-08-16T07:29:49.362293Z",
     "shell.execute_reply": "2025-08-16T07:29:49.361331Z",
     "shell.execute_reply.started": "2025-08-16T07:29:49.343519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "holiday = holiday[~holiday['locale'].isin(['Local', 'Regional'])]\n",
    "holiday = holiday.drop(['locale', 'locale_name', 'description', 'transferred'], axis=1)\n",
    "holiday = holiday[holiday['date'] <= '2017-08-15']\n",
    "holiday = holiday.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:49.363725Z",
     "iopub.status.busy": "2025-08-16T07:29:49.363331Z",
     "iopub.status.idle": "2025-08-16T07:29:49.381753Z",
     "shell.execute_reply": "2025-08-16T07:29:49.380696Z",
     "shell.execute_reply.started": "2025-08-16T07:29:49.363696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "holiday_col = ['type_Additional', 'type_Holiday']\n",
    "holiday[holiday_col] = holiday[holiday_col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_indonesia\n",
    "holiday_indonesia.columns = ['date', 'type']\n",
    "holiday_indonesia = pd.get_dummies(holiday_indonesia, columns=['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_indonesia[holiday_col] = holiday_indonesia[holiday_col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>type_Additional</th>\n",
       "      <th>type_Holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-10-09</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-11-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-11-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2017-05-13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2017-05-14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2017-05-24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>2017-08-10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  type_Additional  type_Holiday\n",
       "0   2012-08-10                0             1\n",
       "1   2012-10-09                0             1\n",
       "2   2012-11-02                0             1\n",
       "3   2012-11-03                0             1\n",
       "4   2012-12-21                1             0\n",
       "..         ...              ...           ...\n",
       "147 2017-05-01                0             1\n",
       "148 2017-05-13                1             0\n",
       "149 2017-05-14                0             1\n",
       "150 2017-05-24                0             1\n",
       "151 2017-08-10                0             1\n",
       "\n",
       "[152 rows x 3 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>type_Additional</th>\n",
       "      <th>type_Holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-09-21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>2025-06-27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2025-12-25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2025-12-26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  type_Additional  type_Holiday\n",
       "0   2017-08-17                0             1\n",
       "1   2017-09-01                0             1\n",
       "2   2017-09-21                0             1\n",
       "3   2017-12-01                0             1\n",
       "4   2017-12-25                0             1\n",
       "..         ...              ...           ...\n",
       "182 2025-06-27                0             1\n",
       "183 2025-08-17                0             1\n",
       "184 2025-09-05                0             1\n",
       "185 2025-12-25                0             1\n",
       "186 2025-12-26                1             0\n",
       "\n",
       "[187 rows x 3 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holiday_indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>type_Additional</th>\n",
       "      <th>type_Holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-10-09</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-11-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-11-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>2025-06-27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>2025-12-25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>2025-12-26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  type_Additional  type_Holiday\n",
       "0   2012-08-10                0             1\n",
       "1   2012-10-09                0             1\n",
       "2   2012-11-02                0             1\n",
       "3   2012-11-03                0             1\n",
       "4   2012-12-21                1             0\n",
       "..         ...              ...           ...\n",
       "328 2025-06-27                0             1\n",
       "329 2025-08-17                0             1\n",
       "330 2025-09-05                0             1\n",
       "331 2025-12-25                0             1\n",
       "332 2025-12-26                1             0\n",
       "\n",
       "[333 rows x 3 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holiday = pd.concat([holiday, holiday_indonesia], ignore_index=True)\n",
    "holiday = holiday.sort_values(['date','type_Additional', 'type_Holiday']).drop_duplicates(subset=['date'], keep='last')\n",
    "holiday = holiday.sort_values('date').reset_index(drop=True)\n",
    "holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:29:49.383359Z",
     "iopub.status.busy": "2025-08-16T07:29:49.383008Z",
     "iopub.status.idle": "2025-08-16T07:29:49.404624Z",
     "shell.execute_reply": "2025-08-16T07:29:49.403722Z",
     "shell.execute_reply.started": "2025-08-16T07:29:49.383327Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\n",
      "List of Holiday Count\n",
      "=======================\n",
      "type_Additional : 90\n",
      "type_Holiday : 243\n"
     ]
    }
   ],
   "source": [
    "print('=======================')\n",
    "print('List of Holiday Count')\n",
    "print('=======================')\n",
    "for col in holiday_col:\n",
    "    print(f'{col} : {len(holiday[holiday[col] == 1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is Ecuador's Dataset that doesn't relate to Indonesian Dataset, thats why we have to remove the states and cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-08-16T07:30:06.148690Z",
     "iopub.status.busy": "2025-08-16T07:30:06.148396Z",
     "iopub.status.idle": "2025-08-16T07:30:06.156349Z",
     "shell.execute_reply": "2025-08-16T07:30:06.155130Z",
     "shell.execute_reply.started": "2025-08-16T07:30:06.148616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "store = store.drop(['city', 'state'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:30:20.340346Z",
     "iopub.status.busy": "2025-08-16T07:30:20.339773Z",
     "iopub.status.idle": "2025-08-16T07:30:31.104417Z",
     "shell.execute_reply": "2025-08-16T07:30:31.103526Z",
     "shell.execute_reply.started": "2025-08-16T07:30:20.340318Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>id</th>\n",
       "      <th>sales</th>\n",
       "      <th>oil</th>\n",
       "      <th>type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>type_Additional</th>\n",
       "      <th>type_Holiday</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>date_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>store_nbr_1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>type_D</td>\n",
       "      <td>cluster_13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>store_nbr_1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>type_D</td>\n",
       "      <td>cluster_13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>store_nbr_1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>type_D</td>\n",
       "      <td>cluster_13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>store_nbr_1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>type_D</td>\n",
       "      <td>cluster_13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>store_nbr_1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>type_D</td>\n",
       "      <td>cluster_13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8216797</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>store_nbr_54</td>\n",
       "      <td>POULTRY</td>\n",
       "      <td>5208650.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>type_C</td>\n",
       "      <td>cluster_3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8216798</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>store_nbr_54</td>\n",
       "      <td>PREPARED FOODS</td>\n",
       "      <td>5208651.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>type_C</td>\n",
       "      <td>cluster_3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8216799</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>store_nbr_54</td>\n",
       "      <td>PRODUCE</td>\n",
       "      <td>5208652.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>type_C</td>\n",
       "      <td>cluster_3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8216800</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>store_nbr_54</td>\n",
       "      <td>SCHOOL AND OFFICE SUPPLIES</td>\n",
       "      <td>5208653.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>type_C</td>\n",
       "      <td>cluster_3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8216801</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>store_nbr_54</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>5208654.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>type_C</td>\n",
       "      <td>cluster_3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8216802 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date     store_nbr                      family         id  \\\n",
       "0       2013-01-01   store_nbr_1                  AUTOMOTIVE        0.0   \n",
       "1       2013-01-01   store_nbr_1                   BABY CARE        1.0   \n",
       "2       2013-01-01   store_nbr_1                      BEAUTY        2.0   \n",
       "3       2013-01-01   store_nbr_1                   BEVERAGES        3.0   \n",
       "4       2013-01-01   store_nbr_1                       BOOKS        4.0   \n",
       "...            ...           ...                         ...        ...   \n",
       "8216797 2025-08-16  store_nbr_54                     POULTRY  5208650.0   \n",
       "8216798 2025-08-16  store_nbr_54              PREPARED FOODS  5208651.0   \n",
       "8216799 2025-08-16  store_nbr_54                     PRODUCE  5208652.0   \n",
       "8216800 2025-08-16  store_nbr_54  SCHOOL AND OFFICE SUPPLIES  5208653.0   \n",
       "8216801 2025-08-16  store_nbr_54                     SEAFOOD  5208654.0   \n",
       "\n",
       "         sales    oil    type     cluster  type_Additional  type_Holiday  day  \\\n",
       "0          NaN  93.12  type_D  cluster_13              0.0           1.0    1   \n",
       "1          NaN  93.12  type_D  cluster_13              0.0           1.0    1   \n",
       "2          NaN  93.12  type_D  cluster_13              0.0           1.0    1   \n",
       "3          NaN  93.12  type_D  cluster_13              0.0           1.0    1   \n",
       "4          NaN  93.12  type_D  cluster_13              0.0           1.0    1   \n",
       "...        ...    ...     ...         ...              ...           ...  ...   \n",
       "8216797    NaN  62.80  type_C   cluster_3              0.0           0.0   16   \n",
       "8216798    NaN  62.80  type_C   cluster_3              0.0           0.0   16   \n",
       "8216799    NaN  62.80  type_C   cluster_3              0.0           0.0   16   \n",
       "8216800    NaN  62.80  type_C   cluster_3              0.0           0.0   16   \n",
       "8216801    NaN  62.80  type_C   cluster_3              0.0           0.0   16   \n",
       "\n",
       "         month  year  day_of_week  day_of_year  week_of_year  date_index  \n",
       "0            1  2013            1            1             1           0  \n",
       "1            1  2013            1            1             1           0  \n",
       "2            1  2013            1            1             1           0  \n",
       "3            1  2013            1            1             1           0  \n",
       "4            1  2013            1            1             1           0  \n",
       "...        ...   ...          ...          ...           ...         ...  \n",
       "8216797      8  2025            5          228            33        4610  \n",
       "8216798      8  2025            5          228            33        4610  \n",
       "8216799      8  2025            5          228            33        4610  \n",
       "8216800      8  2025            5          228            33        4610  \n",
       "8216801      8  2025            5          228            33        4610  \n",
       "\n",
       "[8216802 rows x 17 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine all the datasets\n",
    "data = pd.concat(\n",
    "    [train, test], axis=0, ignore_index=True,\n",
    ").merge(\n",
    "    oil, on=\"date\", how=\"left\",\n",
    ").merge(\n",
    "    store, on=\"store_nbr\", how=\"left\",\n",
    ").merge(\n",
    "    holiday, on=\"date\", how=\"left\",\n",
    ").sort_values([\"date\", \"store_nbr\", \"family\"], ignore_index=True)\n",
    "\n",
    "# fill columns with 0s to indicate absence of holidays/events\n",
    "data[holiday.columns] = data[holiday.columns].fillna(0)\n",
    "\n",
    "# include date-related future covariates\n",
    "data[\"day\"] = data.date.dt.day\n",
    "data[\"month\"] = data.date.dt.month\n",
    "data[\"year\"] = data.date.dt.year\n",
    "data[\"day_of_week\"] = data.date.dt.dayofweek\n",
    "data[\"day_of_year\"] = data.date.dt.dayofyear\n",
    "data[\"week_of_year\"] = data.date.dt.isocalendar().week.astype(int)\n",
    "data[\"date_index\"] = data.date.factorize()[0] # sort by date above before computing this\n",
    "\n",
    "# to impute days with zero sales using linear interpolation later\n",
    "zero_sales_dates = missing_dates + [f\"{j}-01-01\" for j in range(2013, 2018)]\n",
    "data.loc[(data.date.isin(zero_sales_dates))&(data.sales.eq(0)), [\"sales\"]] = np.nan\n",
    "\n",
    "# add prefixes for clarity\n",
    "data.store_nbr = data.store_nbr.apply(lambda x: (f\"store_nbr_{x}\"))\n",
    "data.cluster = data.cluster.apply(lambda x: (f\"cluster_{x}\"))\n",
    "data.type = data.type.apply(lambda x: (f\"type_{x}\"))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>family</th>\n",
       "      <th>type</th>\n",
       "      <th>sales</th>\n",
       "      <th>oil</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>date_index</th>\n",
       "      <th>type_Additional</th>\n",
       "      <th>type_Holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760810</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760811</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760812</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760813</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760814</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>760815 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date      family    type  sales    oil  day  month  year  \\\n",
       "0      2013-01-01  AUTOMOTIVE  type_A    NaN  93.12    1      1  2013   \n",
       "1      2013-01-01  AUTOMOTIVE  type_B    NaN  93.12    1      1  2013   \n",
       "2      2013-01-01  AUTOMOTIVE  type_C    NaN  93.12    1      1  2013   \n",
       "3      2013-01-01  AUTOMOTIVE  type_D    NaN  93.12    1      1  2013   \n",
       "4      2013-01-01  AUTOMOTIVE  type_E    NaN  93.12    1      1  2013   \n",
       "...           ...         ...     ...    ...    ...  ...    ...   ...   \n",
       "760810 2025-08-16     SEAFOOD  type_A    NaN  62.80   16      8  2025   \n",
       "760811 2025-08-16     SEAFOOD  type_B    NaN  62.80   16      8  2025   \n",
       "760812 2025-08-16     SEAFOOD  type_C    NaN  62.80   16      8  2025   \n",
       "760813 2025-08-16     SEAFOOD  type_D    NaN  62.80   16      8  2025   \n",
       "760814 2025-08-16     SEAFOOD  type_E    NaN  62.80   16      8  2025   \n",
       "\n",
       "        day_of_week  day_of_year  week_of_year  date_index  type_Additional  \\\n",
       "0                 1            1             1           0              0.0   \n",
       "1                 1            1             1           0              0.0   \n",
       "2                 1            1             1           0              0.0   \n",
       "3                 1            1             1           0              0.0   \n",
       "4                 1            1             1           0              0.0   \n",
       "...             ...          ...           ...         ...              ...   \n",
       "760810            5          228            33        4610              0.0   \n",
       "760811            5          228            33        4610              0.0   \n",
       "760812            5          228            33        4610              0.0   \n",
       "760813            5          228            33        4610              0.0   \n",
       "760814            5          228            33        4610              0.0   \n",
       "\n",
       "        type_Holiday  \n",
       "0                1.0  \n",
       "1                1.0  \n",
       "2                1.0  \n",
       "3                1.0  \n",
       "4                1.0  \n",
       "...              ...  \n",
       "760810           0.0  \n",
       "760811           0.0  \n",
       "760812           0.0  \n",
       "760813           0.0  \n",
       "760814           0.0  \n",
       "\n",
       "[760815 rows x 14 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# buat aturan agregasi\n",
    "agg_rules = {\n",
    "    \"sales\": \"mean\",   # rata-rata untuk sales\n",
    "    \"oil\": \"first\",    # misalnya ambil nilai pertama (karena biasanya sama per tanggal)\n",
    "    \"day\": \"first\",\n",
    "    \"month\": \"first\",\n",
    "    \"year\": \"first\",\n",
    "    \"day_of_week\": \"first\",\n",
    "    \"day_of_year\": \"first\",\n",
    "    \"week_of_year\": \"first\",\n",
    "    \"date_index\": \"first\",\n",
    "    \"type_Additional\": \"first\",\n",
    "    \"type_Holiday\": \"first\"\n",
    "}\n",
    "\n",
    "# lakukan groupby dan simpan ke data baru\n",
    "data = (\n",
    "    data.groupby([\"date\", \"family\", \"type\"], as_index=False)\n",
    "        .agg(agg_rules)\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the transformation pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on, we begin exploring the functionalities of the Darts library for forecasting. We define the transformation pipelines here, which includes:\n",
    "* `MissingValuesFiller` to fill missing values like the NaN values we temporarily set previously. The default behavior is linear interpolation.\n",
    "* `StaticCovariatesTransformer` to perform encoding/scaling of our static covariates. Our static covariates are all categorical, so we specify to perform one-hot encoding using `OneHotEncoder` from sklearn.\n",
    "* `InvertibleMapper` to define a custom log transformer for our target series. Log transforming may help to stabilize our target series by reducing the magnitude of large values.\n",
    "* `Scaler` to perform scaling for all our target series and covariates. The default behavior is min-max scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:30:31.545066Z",
     "iopub.status.busy": "2025-08-16T07:30:31.544803Z",
     "iopub.status.idle": "2025-08-16T07:30:31.551611Z",
     "shell.execute_reply": "2025-08-16T07:30:31.550688Z",
     "shell.execute_reply.started": "2025-08-16T07:30:31.545036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_pipeline(static_covs_transform=False, log_transform=False):\n",
    "    lst = []\n",
    "    \n",
    "    # fill missing values\n",
    "    filler = MissingValuesFiller(n_jobs=-1)\n",
    "    lst.append(filler)\n",
    "    \n",
    "    # specify transformation for static covariates\n",
    "    if static_covs_transform:\n",
    "        static_covs_transformer = StaticCovariatesTransformer(\n",
    "            transformer_cat=OneHotEncoder(),\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        lst.append(static_covs_transformer)\n",
    "\n",
    "    # perform log transformation on sales\n",
    "    if log_transform:\n",
    "        log_transformer = InvertibleMapper(\n",
    "            fn=np.log1p,\n",
    "            inverse_fn=np.expm1,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        lst.append(log_transformer)\n",
    "\n",
    "    # rescale time series\n",
    "    scaler = Scaler()\n",
    "    lst.append(scaler)\n",
    "\n",
    "    # chain all transformations\n",
    "    pipeline = Pipeline(lst)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the target series together with the static covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>family</th>\n",
       "      <th>type</th>\n",
       "      <th>sales</th>\n",
       "      <th>oil</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>date_index</th>\n",
       "      <th>type_Additional</th>\n",
       "      <th>type_Holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760810</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760811</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760812</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760813</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760814</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>760815 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date      family    type  sales    oil  day  month  year  \\\n",
       "0      2013-01-01  AUTOMOTIVE  type_A    NaN  93.12    1      1  2013   \n",
       "1      2013-01-01  AUTOMOTIVE  type_B    NaN  93.12    1      1  2013   \n",
       "2      2013-01-01  AUTOMOTIVE  type_C    NaN  93.12    1      1  2013   \n",
       "3      2013-01-01  AUTOMOTIVE  type_D    NaN  93.12    1      1  2013   \n",
       "4      2013-01-01  AUTOMOTIVE  type_E    NaN  93.12    1      1  2013   \n",
       "...           ...         ...     ...    ...    ...  ...    ...   ...   \n",
       "760810 2025-08-16     SEAFOOD  type_A    NaN  62.80   16      8  2025   \n",
       "760811 2025-08-16     SEAFOOD  type_B    NaN  62.80   16      8  2025   \n",
       "760812 2025-08-16     SEAFOOD  type_C    NaN  62.80   16      8  2025   \n",
       "760813 2025-08-16     SEAFOOD  type_D    NaN  62.80   16      8  2025   \n",
       "760814 2025-08-16     SEAFOOD  type_E    NaN  62.80   16      8  2025   \n",
       "\n",
       "        day_of_week  day_of_year  week_of_year  date_index  type_Additional  \\\n",
       "0                 1            1             1           0              0.0   \n",
       "1                 1            1             1           0              0.0   \n",
       "2                 1            1             1           0              0.0   \n",
       "3                 1            1             1           0              0.0   \n",
       "4                 1            1             1           0              0.0   \n",
       "...             ...          ...           ...         ...              ...   \n",
       "760810            5          228            33        4610              0.0   \n",
       "760811            5          228            33        4610              0.0   \n",
       "760812            5          228            33        4610              0.0   \n",
       "760813            5          228            33        4610              0.0   \n",
       "760814            5          228            33        4610              0.0   \n",
       "\n",
       "        type_Holiday  \n",
       "0                1.0  \n",
       "1                1.0  \n",
       "2                1.0  \n",
       "3                1.0  \n",
       "4                1.0  \n",
       "...              ...  \n",
       "760810           0.0  \n",
       "760811           0.0  \n",
       "760812           0.0  \n",
       "760813           0.0  \n",
       "760814           0.0  \n",
       "\n",
       "[760815 rows x 14 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['store_nbr', 'cluster'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:30:31.552923Z",
     "iopub.status.busy": "2025-08-16T07:30:31.552570Z",
     "iopub.status.idle": "2025-08-16T07:30:31.573704Z",
     "shell.execute_reply": "2025-08-16T07:30:31.572719Z",
     "shell.execute_reply.started": "2025-08-16T07:30:31.552852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_target_series(static_cols, log_transform=True):    \n",
    "    target_dict = {}\n",
    "    pipe_dict = {}\n",
    "    id_dict = {}\n",
    "\n",
    "    # Aturan agregasi: gunakan rata-rata untuk sales\n",
    "    agg_rules = {\"sales\": \"mean\"}\n",
    "    for col in static_cols:\n",
    "        if col not in [\"type\"]:   # cegah duplikat\n",
    "            agg_rules[col] = \"first\"\n",
    "\n",
    "    for fam in tqdm_notebook(data.family.unique(), desc=\"Extracting target series\"):\n",
    "        df_fam = data[(data.family.eq(fam)) & (data.date.le(train_end.strftime(\"%Y-%m-%d\")))]\n",
    "        \n",
    "        # Agregasi per type dan date\n",
    "        df_agg = df_fam.groupby([\"type\", \"date\"]).agg(agg_rules).reset_index()\n",
    "        \n",
    "        pipe = get_pipeline(True, log_transform=log_transform)\n",
    "        \n",
    "        target = TimeSeries.from_group_dataframe(\n",
    "            df=df_agg,\n",
    "            time_col=\"date\",\n",
    "            value_cols=\"sales\",\n",
    "            group_cols=\"type\",\n",
    "            static_cols=static_cols,\n",
    "            freq=\"D\",\n",
    "            fill_missing_dates=True,\n",
    "            fillna_value=0\n",
    "        )\n",
    "\n",
    "        target_id = [{\"type\": t.static_covariates.type[0], \"family\": fam} \n",
    "                     for t in target]\n",
    "        id_dict[fam] = target_id\n",
    "        \n",
    "        target = pipe.fit_transform(target)\n",
    "        target_dict[fam] = [t.astype(np.float32) for t in target]\n",
    "        pipe_dict[fam] = pipe[2:]\n",
    "        \n",
    "    return target_dict, pipe_dict, id_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:30:31.574852Z",
     "iopub.status.busy": "2025-08-16T07:30:31.574578Z",
     "iopub.status.idle": "2025-08-16T07:31:35.753779Z",
     "shell.execute_reply": "2025-08-16T07:31:35.752698Z",
     "shell.execute_reply.started": "2025-08-16T07:30:31.574829Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba8acc9ccf24c009354f7f0bd4cba0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting target series:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "static_cols = []\n",
    "\n",
    "target_dict, pipe_dict, id_dict = get_target_series(static_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the past and future covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>family</th>\n",
       "      <th>type</th>\n",
       "      <th>sales</th>\n",
       "      <th>oil</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>date_index</th>\n",
       "      <th>type_Additional</th>\n",
       "      <th>type_Holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760810</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760811</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760812</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760813</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760814</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>760815 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date      family    type  sales    oil  day  month  year  \\\n",
       "0      2013-01-01  AUTOMOTIVE  type_A    NaN  93.12    1      1  2013   \n",
       "1      2013-01-01  AUTOMOTIVE  type_B    NaN  93.12    1      1  2013   \n",
       "2      2013-01-01  AUTOMOTIVE  type_C    NaN  93.12    1      1  2013   \n",
       "3      2013-01-01  AUTOMOTIVE  type_D    NaN  93.12    1      1  2013   \n",
       "4      2013-01-01  AUTOMOTIVE  type_E    NaN  93.12    1      1  2013   \n",
       "...           ...         ...     ...    ...    ...  ...    ...   ...   \n",
       "760810 2025-08-16     SEAFOOD  type_A    NaN  62.80   16      8  2025   \n",
       "760811 2025-08-16     SEAFOOD  type_B    NaN  62.80   16      8  2025   \n",
       "760812 2025-08-16     SEAFOOD  type_C    NaN  62.80   16      8  2025   \n",
       "760813 2025-08-16     SEAFOOD  type_D    NaN  62.80   16      8  2025   \n",
       "760814 2025-08-16     SEAFOOD  type_E    NaN  62.80   16      8  2025   \n",
       "\n",
       "        day_of_week  day_of_year  week_of_year  date_index  type_Additional  \\\n",
       "0                 1            1             1           0              0.0   \n",
       "1                 1            1             1           0              0.0   \n",
       "2                 1            1             1           0              0.0   \n",
       "3                 1            1             1           0              0.0   \n",
       "4                 1            1             1           0              0.0   \n",
       "...             ...          ...           ...         ...              ...   \n",
       "760810            5          228            33        4610              0.0   \n",
       "760811            5          228            33        4610              0.0   \n",
       "760812            5          228            33        4610              0.0   \n",
       "760813            5          228            33        4610              0.0   \n",
       "760814            5          228            33        4610              0.0   \n",
       "\n",
       "        type_Holiday  \n",
       "0                1.0  \n",
       "1                1.0  \n",
       "2                1.0  \n",
       "3                1.0  \n",
       "4                1.0  \n",
       "...              ...  \n",
       "760810           0.0  \n",
       "760811           0.0  \n",
       "760812           0.0  \n",
       "760813           0.0  \n",
       "760814           0.0  \n",
       "\n",
       "[760815 rows x 14 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:31:39.620559Z",
     "iopub.status.busy": "2025-08-16T07:31:39.620268Z",
     "iopub.status.idle": "2025-08-16T07:32:20.124306Z",
     "shell.execute_reply": "2025-08-16T07:32:20.123280Z",
     "shell.execute_reply.started": "2025-08-16T07:31:39.620538Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mengekstrak covariates dari data yang sudah diagregasi...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a765e19ba5490f84e74621f5c038eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting covariates:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n",
      "UserWarning: The (time) index from `df` is monotonically increasing. This results in time series groups with non-overlapping (time) index. You can ignore this warning if the index represents the actual index of each individual time series group.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ekstraksi covariates selesai tanpa error.\n"
     ]
    }
   ],
   "source": [
    "def get_covariates(\n",
    "    df_input, # <-- Menerima dataframe sebagai input\n",
    "    future_cols,\n",
    "    past_ma_cols=None,\n",
    "    future_ma_cols=None,\n",
    "    past_window_sizes=[7, 28],\n",
    "    future_window_sizes=[7, 28],\n",
    "):\n",
    "    past_dict = {}\n",
    "    future_dict = {}\n",
    "    \n",
    "    covs_pipe = get_pipeline() # Asumsi fungsi get_pipeline() sudah ada\n",
    "\n",
    "    for fam in tqdm_notebook(df_input.family.unique(), desc=\"Extracting covariates\"):\n",
    "        df = df_input[df_input.family.eq(fam)]\n",
    "        \n",
    "        # Ekstrak past covariates\n",
    "        past_covs = TimeSeries.from_group_dataframe(\n",
    "            df=df[df.date.le(train_end.strftime(\"%Y-%m-%d\"))],\n",
    "            time_col=\"date\",\n",
    "            value_cols=[],  # kosong\n",
    "            group_cols=\"type\",\n",
    "            freq='D',\n",
    "            fill_missing_dates=True,\n",
    "            fillna_value=0\n",
    "        )\n",
    "        past_covs = [p.with_static_covariates(None) for p in past_covs]\n",
    "        # past_covs = covs_pipe.fit_transform(past_covs)\n",
    "        \n",
    "        # if past_ma_cols is not None:\n",
    "        #     for size in past_window_sizes:\n",
    "        #         ma_filter = MovingAverageFilter(window=size)\n",
    "        #         old_names = [f\"rolling_mean_{size}_{col}\" for col in past_ma_cols]\n",
    "        #         new_names = [f\"{col}_ma{size}\" for col in past_ma_cols]\n",
    "        #         past_ma_covs = [\n",
    "        #             ma_filter.filter(p[past_ma_cols]).with_columns_renamed(old_names, new_names) \n",
    "        #             for p in past_covs\n",
    "        #         ]\n",
    "        #         past_covs = [p.stack(p_ma) for p, p_ma in zip(past_covs, past_ma_covs)]\n",
    "\n",
    "        if past_ma_cols is not None and len(past_ma_cols) > 0:\n",
    "            past_covs = TimeSeries.from_group_dataframe(\n",
    "                df=df[df.date.le(train_end.strftime(\"%Y-%m-%d\"))],\n",
    "                time_col=\"date\",\n",
    "                value_cols=past_ma_cols,\n",
    "                group_cols=\"type\",\n",
    "                freq='D',\n",
    "                fill_missing_dates=True,\n",
    "                fillna_value=0 \n",
    "            )\n",
    "            past_covs = [p.with_static_covariates(None) for p in past_covs]\n",
    "            past_covs = covs_pipe.fit_transform(past_covs)\n",
    "        else:\n",
    "            # Tidak ada past covariates → tetap buat TimeSeries kosong tapi tidak pakai pipeline\n",
    "            past_covs = TimeSeries.from_group_dataframe(\n",
    "                df=df[df.date.le(train_end.strftime(\"%Y-%m-%d\"))],\n",
    "                time_col=\"date\",\n",
    "                value_cols=[],  # buat dummy kolom\n",
    "                group_cols=\"type\",\n",
    "                freq='D',\n",
    "                fill_missing_dates=True,\n",
    "                fillna_value=0 \n",
    "            )\n",
    "            past_covs = [p.with_static_covariates(None) for p in past_covs]\n",
    "        \n",
    "        past_dict[fam] = [p.astype(np.float32) for p in past_covs]\n",
    "        if all([p.n_components == 0 for p in past_covs]):\n",
    "            past_dict[fam] = None\n",
    "\n",
    "        # Ekstrak future covariates\n",
    "        future_covs = TimeSeries.from_group_dataframe(\n",
    "            df=df,\n",
    "            time_col=\"date\",\n",
    "            value_cols=future_cols,\n",
    "            group_cols=\"type\",\n",
    "            freq='D',\n",
    "            fill_missing_dates=True,\n",
    "            fillna_value=0 \n",
    "        )\n",
    "        future_covs = [f.with_static_covariates(None) for f in future_covs]\n",
    "        future_covs = covs_pipe.fit_transform(future_covs)\n",
    "\n",
    "        if future_ma_cols is not None:\n",
    "            if future_ma_cols is not None:\n",
    "                for size in future_window_sizes:\n",
    "                    ma_filter = MovingAverageFilter(window=size)\n",
    "                    old_names = [f\"rolling_mean_{size}_{col}\" for col in future_ma_cols]\n",
    "                    new_names = [f\"{col}_ma{size}\" for col in future_ma_cols]\n",
    "                    future_ma_covs = [\n",
    "                        ma_filter.filter(f[future_ma_cols]).with_columns_renamed(old_names, new_names) \n",
    "                        for f in future_covs\n",
    "                    ]\n",
    "                    future_covs = [f.stack(f_ma) for f, f_ma in zip(future_covs, future_ma_covs)]\n",
    "        \n",
    "        future_dict[fam] = [f.astype(np.float32) for f in future_covs]\n",
    "            \n",
    "    return past_dict, future_dict\n",
    "\n",
    "future_cols = [\"oil\", \"day\", \"month\", \"year\", \"day_of_week\", \"day_of_year\", \"week_of_year\", \"date_index\", \"type_Holiday\", \"type_Additional\"]\n",
    "past_ma_cols = None\n",
    "future_ma_cols = [\"oil\"]\n",
    "\n",
    "print(\"\\nMengekstrak covariates dari data yang sudah diagregasi...\")\n",
    "agg_dict = {\n",
    "    'sales': 'sum',\n",
    "    'oil': 'mean',\n",
    "    'day': 'first',\n",
    "    'month': 'first',\n",
    "    'year': 'first',\n",
    "    'day_of_week': 'first',\n",
    "    'day_of_year': 'first',\n",
    "    'week_of_year': 'first',\n",
    "    'date_index': 'first',\n",
    "    'type_Additional': 'max',\n",
    "    'type_Holiday': 'max',\n",
    "    'type': 'first'\n",
    "}\n",
    "\n",
    "non_agg_cols = [\"date\", \"family\", \"type\"]\n",
    "data = data.groupby(non_agg_cols, as_index=False).agg(agg_dict)\n",
    "\n",
    "\n",
    "past_dict, future_dict = get_covariates(\n",
    "    data,\n",
    "    future_cols, \n",
    "    past_ma_cols, \n",
    "    future_ma_cols\n",
    ")\n",
    "\n",
    "print(\"\\nEkstraksi covariates selesai tanpa error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the model trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-08-16T07:32:20.125525Z",
     "iopub.status.busy": "2025-08-16T07:32:20.125279Z",
     "iopub.status.idle": "2025-08-16T07:32:20.130880Z",
     "shell.execute_reply": "2025-08-16T07:32:20.130133Z",
     "shell.execute_reply.started": "2025-08-16T07:32:20.125505Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\n",
      "List of all covariates:\n",
      "=======================\n",
      "Static covariates:\n",
      "[]\n",
      "\n",
      "\n",
      "Future covariates:\n",
      "['oil', 'day', 'month', 'year', 'day_of_week', 'day_of_year', 'week_of_year', 'date_index', 'type_Holiday', 'type_Additional', 'oil_ma7', 'oil_ma28']\n"
     ]
    }
   ],
   "source": [
    "cprint(\n",
    "    \"List of all covariates:\",\n",
    "    \"Static covariates:\",\n",
    "    static_cols,\n",
    "    \"\",\n",
    "    # \"Past covariates:\",\n",
    "    # past_dict[\"AUTOMOTIVE\"][0].components.tolist(),\n",
    "    \"\",\n",
    "    \"Future covariates:\",\n",
    "    future_dict[\"AUTOMOTIVE\"][0].components.tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (data['date'] >= \"2017-08-16\") & (data['date'] <= \"2025-08-16\")\n",
    "subset = data.loc[mask]\n",
    "\n",
    "forecast_horizon = len(subset.drop_duplicates(subset='date', keep='first'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:32:20.132109Z",
     "iopub.status.busy": "2025-08-16T07:32:20.131754Z",
     "iopub.status.idle": "2025-08-16T07:32:20.149090Z",
     "shell.execute_reply": "2025-08-16T07:32:20.148269Z",
     "shell.execute_reply.started": "2025-08-16T07:32:20.132087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAINER_CONFIG = {\n",
    "    # the time series data previously extracted\n",
    "    \"target_dict\": target_dict,\n",
    "    \"pipe_dict\": pipe_dict,\n",
    "    \"id_dict\": id_dict,\n",
    "    \"past_dict\": past_dict,\n",
    "    \"future_dict\": future_dict,\n",
    "    \n",
    "    # time series cross-validation using a rolling forecasting origin\n",
    "    \"forecast_horizon\": forecast_horizon, # the length of the validation set\n",
    "    \"folds\": 1, # the number of training sets (setting to 1 means the standard train-validation split)\n",
    "    \n",
    "    # the number of previous days to check for zero sales; if all are zero, generate zero forecasts\n",
    "    \"zero_fc_window\": 21,\n",
    "    \n",
    "    # specify the covariates in a list to include in the model\n",
    "    # set to None to not use any, and set to 'keep_all' to include everything\n",
    "    \"static_covs\": \"keep_all\", # specify from ['city', 'state', 'cluster', 'type', 'store_nbr'], will extract all one-hot encoded columns\n",
    "    \"past_covs\": \"keep_all\",\n",
    "    \"future_covs\": \"keep_all\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:32:20.150567Z",
     "iopub.status.busy": "2025-08-16T07:32:20.150229Z",
     "iopub.status.idle": "2025-08-16T07:32:20.183028Z",
     "shell.execute_reply": "2025-08-16T07:32:20.181969Z",
     "shell.execute_reply.started": "2025-08-16T07:32:20.150538Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_dict,\n",
    "        pipe_dict,\n",
    "        id_dict,\n",
    "        past_dict,\n",
    "        future_dict,\n",
    "        forecast_horizon,\n",
    "        folds,\n",
    "        zero_fc_window,\n",
    "        static_covs=None,\n",
    "        past_covs=None,\n",
    "        future_covs=None,\n",
    "    ):\n",
    "        self.target_dict = target_dict.copy()\n",
    "        self.pipe_dict = pipe_dict.copy()\n",
    "        self.id_dict = id_dict.copy()\n",
    "        self.past_dict = past_dict.copy()\n",
    "        self.future_dict = future_dict.copy()\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.folds = folds\n",
    "        self.zero_fc_window = zero_fc_window\n",
    "        self.static_covs = static_covs\n",
    "        self.past_covs = past_covs\n",
    "        self.future_covs = future_covs\n",
    "        self.last_fitted_models_ = {}\n",
    "        \n",
    "        # set up time series data\n",
    "        self.setup()\n",
    "    \n",
    "    def setup(self):\n",
    "        for fam in tqdm_notebook(self.target_dict.keys(), desc=\"Setting up\"):\n",
    "            # keep the specified static covariates\n",
    "            if self.static_covs != \"keep_all\":\n",
    "                if self.static_covs is not None:\n",
    "                    target = self.target_dict[fam]\n",
    "                    keep_static = [col for col in target[0].static_covariates.columns if col.startswith(tuple(self.static_covs))]\n",
    "                    static_covs_df = [t.static_covariates[keep_static] for t in target]\n",
    "                    self.target_dict[fam] = [t.with_static_covariates(d) for t, d in zip(target, static_covs_df)]\n",
    "                else:\n",
    "                    self.target_dict[fam] = [t.with_static_covariates(None) for t in target]\n",
    "            \n",
    "            # keep the specified past covariates\n",
    "            if self.past_covs != \"keep_all\":\n",
    "                if self.past_covs is not None:\n",
    "                    self.past_dict[fam] = [p[self.past_covs] for p in self.past_dict[fam]]\n",
    "                else:\n",
    "                    self.past_dict[fam] = None\n",
    "                \n",
    "            # keep the specified future covariates\n",
    "            if self.future_covs != \"keep_all\":\n",
    "                if self.future_covs is not None:\n",
    "                    self.future_dict[fam] = [p[self.future_covs] for p in self.future_dict[fam]]\n",
    "                else:\n",
    "                    self.future_dict[fam] = None\n",
    "    \n",
    "    def clip(self, array):\n",
    "        return np.clip(array, a_min=0., a_max=None)\n",
    "    \n",
    "    def train_valid_split(self, target, length):\n",
    "        train = [t[:-length] for t in target]\n",
    "        valid_end_idx = -length + self.forecast_horizon\n",
    "        if valid_end_idx >= 0:\n",
    "            valid_end_idx = None\n",
    "        valid = [t[-length:valid_end_idx] for t in target]\n",
    "        \n",
    "        return train, valid\n",
    "    \n",
    "    def get_models(self, model_names, model_configs):\n",
    "        models = {\n",
    "            \"lr\": LinearRegressionModel,\n",
    "            \"lgbm\": LightGBMModel,\n",
    "            \"cat\": CatBoostModel,\n",
    "            \"xgb\": XGBModel,\n",
    "        }\n",
    "        assert isinstance(model_names, list) and isinstance(model_configs, list),\\\n",
    "        \"Both the model names and model configurations must be specified in lists.\"\n",
    "        assert all(name in models for name in model_names),\\\n",
    "        f\"Model names '{model_names}' not recognized.\"\n",
    "        assert len(model_names) == len(model_configs),\\\n",
    "        \"The number of model names and the number of model configurations do not match.\"\n",
    "        \n",
    "        if \"xgb\" in model_names:\n",
    "            xgb_idx = np.where(np.array(model_names)==\"xgb\")[0]\n",
    "            for idx in xgb_idx:\n",
    "                # change to histogram-based method for XGBoost to get faster training time\n",
    "                model_configs[idx] = {\"tree_method\": \"hist\", **model_configs[idx]}\n",
    "        \n",
    "        return [models[name](**model_configs[j]) for j, name in enumerate(model_names)]\n",
    "    \n",
    "    def generate_forecasts(self, models, train, pipe, past_covs, future_covs, drop_before):\n",
    "        if drop_before is not None:\n",
    "            date = pd.Timestamp(drop_before) - pd.Timedelta(days=1)\n",
    "            train = [t.drop_before(date) for t in train]\n",
    "        inputs = {\n",
    "            \"series\": train,\n",
    "            \"past_covariates\": past_covs,\n",
    "            \"future_covariates\": future_covs,\n",
    "        }\n",
    "        zero_pred = pd.DataFrame({\n",
    "            \"date\": pd.date_range(train[0].end_time(), periods=self.forecast_horizon+1)[1:],\n",
    "            \"sales\": np.zeros(self.forecast_horizon),\n",
    "        })\n",
    "        zero_pred = TimeSeries.from_dataframe(\n",
    "            df=zero_pred,\n",
    "            time_col=\"date\",\n",
    "            value_cols=\"sales\",\n",
    "        )\n",
    "        \n",
    "        pred_list = []\n",
    "        ens_pred = [0 for _ in range(len(train))]\n",
    "        \n",
    "        for m in models:\n",
    "            # fit training data to model\n",
    "            m.fit(**inputs)\n",
    "\n",
    "            # generate forecasts and apply inverse transformations\n",
    "            pred = m.predict(n=self.forecast_horizon, **inputs)\n",
    "            pred = pipe.inverse_transform(pred)\n",
    "\n",
    "            # set zero forecasts for target series where the recent observations are 0s\n",
    "            for j in range(len(train)):\n",
    "                if train[j][-self.zero_fc_window:].values().sum() == 0:\n",
    "                    pred[j] = zero_pred\n",
    "            \n",
    "            # clip negative forecasts to 0s\n",
    "            pred = [p.map(self.clip) for p in pred]\n",
    "            pred_list.append(pred)\n",
    "            \n",
    "            # ensemble averaging\n",
    "            for j in range(len(ens_pred)):\n",
    "                ens_pred[j] += pred[j] / len(models)\n",
    "\n",
    "        return pred_list, ens_pred, models\n",
    "    \n",
    "    def metric(self, valid, pred):\n",
    "        return float(np.mean(rmsle(valid, pred)))\n",
    "    \n",
    "    def validate(self, model_names, model_configs, drop_before=None):\n",
    "        # helper value to align printed text below\n",
    "        longest_len = len(max(self.target_dict.keys(), key=len))\n",
    "        \n",
    "        # store metric values for each model\n",
    "        model_metrics_history = []\n",
    "        ens_metric_history = []\n",
    "        \n",
    "        for fam in tqdm_notebook(self.target_dict, desc=\"Performing validation\"):\n",
    "            target = self.target_dict[fam]\n",
    "            pipe = self.pipe_dict[fam]\n",
    "            past_covs = self.past_dict[fam]\n",
    "            future_covs = self.future_dict[fam]\n",
    "            \n",
    "            # record average metric value over all folds\n",
    "            model_metrics = []\n",
    "            ens_metric = 0\n",
    "            \n",
    "            for j in range(self.folds):    \n",
    "                # perform train-validation split and apply transformations\n",
    "                length = (self.folds - j) * self.forecast_horizon\n",
    "                train, valid = self.train_valid_split(target, length)\n",
    "                valid = pipe.inverse_transform(valid)\n",
    "\n",
    "                # generate forecasts and compute metric\n",
    "                models = self.get_models(model_names, model_configs)\n",
    "                pred_list, ens_pred = self.generate_forecasts(models, train, pipe, past_covs, future_covs, drop_before)\n",
    "                metric_list = [self.metric(valid, pred) / self.folds for pred in pred_list]\n",
    "                model_metrics.append(metric_list)\n",
    "                if len(models) > 1:\n",
    "                    ens_metric_fold = self.metric(valid, ens_pred) / self.folds\n",
    "                    ens_metric += ens_metric_fold\n",
    "                \n",
    "            # store final metric value for each model\n",
    "            model_metrics = np.sum(model_metrics, axis=0)\n",
    "            model_metrics_history.append(model_metrics)\n",
    "            ens_metric_history.append(ens_metric)\n",
    "            \n",
    "            # print metric value for each family\n",
    "            print(\n",
    "                fam,\n",
    "                \" \" * (longest_len - len(fam)),\n",
    "                \" | \",\n",
    "                \" - \".join([f\"{model}: {metric:.5f}\" for model, metric in zip(model_names, model_metrics)]),\n",
    "                f\" - ens: {ens_metric:.5f}\" if len(models) > 1 else \"\",\n",
    "                sep=\"\",\n",
    "            )\n",
    "            \n",
    "        # print overall metric value\n",
    "        cprint(\n",
    "            \"Average RMSLE | \"\n",
    "            + \" - \".join([f\"{model}: {metric:.5f}\" \n",
    "                          for model, metric in zip(model_names, np.mean(model_metrics_history, axis=0))])\n",
    "            + (f\" - ens: {np.mean(ens_metric_history):.5f}\" if len(models) > 1 else \"\"),\n",
    "        )\n",
    "        \n",
    "    def ensemble_predict(self, model_names, model_configs, drop_before=None):\n",
    "        forecasts = []\n",
    "        self.last_fitted_models_ = {} \n",
    "\n",
    "        for fam in tqdm_notebook(self.target_dict.keys(), desc=\"Generating forecasts\"):\n",
    "            target = self.target_dict[fam]\n",
    "            pipe = self.pipe_dict[fam]\n",
    "            target_id = self.id_dict[fam]\n",
    "            past_covs = self.past_dict.get(fam)\n",
    "            future_covs = self.future_dict.get(fam)\n",
    "\n",
    "            models = self.get_models(model_names, model_configs)\n",
    "            \n",
    "            # Unpacking sudah benar\n",
    "            # Di dalam fungsi ensemble_predict, sebelum baris yang error\n",
    "\n",
    "            pred_list, ens_pred, trained_models = self.generate_forecasts(models, target, pipe, past_covs, future_covs, drop_before)\n",
    "            self.last_fitted_models_[fam] = trained_models\n",
    "            \n",
    "            # --- TAMBAHKAN KODE DEBUG INI ---\n",
    "            if ens_pred: # Jika ens_pred tidak kosong\n",
    "                print(\"Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\")\n",
    "                print(dir(ens_pred[0]))\n",
    "            # -------------------------------\n",
    "\n",
    "            # Baris asli Anda yang menyebabkan error\n",
    "            ens_pred = [p.to_dataframe().reset_index().assign(**i) for p, i in zip(ens_pred, target_id)]\n",
    "            # ------------------------------------\n",
    "\n",
    "            ens_pred = pd.concat(ens_pred, axis=0)\n",
    "            forecasts.append(ens_pred)\n",
    "\n",
    "        forecasts = pd.concat(forecasts, axis=0)\n",
    "        forecasts = forecasts.rename_axis(None, axis=1)\n",
    "\n",
    "        if \"date\" in forecasts.columns:\n",
    "            forecasts = forecasts.reset_index(drop=True)  # jangan bikin kolom date lagi\n",
    "        else:\n",
    "            forecasts = forecasts.reset_index(names=\"date\")\n",
    "\n",
    "        return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tanggal terakhir future covariates: 2025-08-16 00:00:00\n",
      "Forecast horizon aman: 2909\n"
     ]
    }
   ],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"random_state\": 0,\n",
    "    \"lags\": 63,\n",
    "    \"lags_past_covariates\": None,  \n",
    "    \"lags_future_covariates\": list(range(1, 15)) if TRAINER_CONFIG[\"future_covs\"] is not None else None,\n",
    "    \"output_chunk_length\": 1,\n",
    "}\n",
    "\n",
    "max_future_lag = max(BASE_CONFIG[\"lags_future_covariates\"])  # misal 14\n",
    "some_fam = list(future_dict.keys())[0]\n",
    "last_date_future_covs = future_dict[some_fam][0].end_time()\n",
    "print(\"Tanggal terakhir future covariates:\", last_date_future_covs)\n",
    "last_date_target = target_dict[some_fam][0].end_time()\n",
    "forecast_horizon_safe = (last_date_future_covs - last_date_target).days - max_future_lag\n",
    "print(\"Forecast horizon aman:\", forecast_horizon_safe)\n",
    "TRAINER_CONFIG[\"forecast_horizon\"] = forecast_horizon_safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T07:32:20.184780Z",
     "iopub.status.busy": "2025-08-16T07:32:20.184058Z",
     "iopub.status.idle": "2025-08-16T07:32:20.518852Z",
     "shell.execute_reply": "2025-08-16T07:32:20.517795Z",
     "shell.execute_reply.started": "2025-08-16T07:32:20.184752Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aab9eb2c610484d99725bf6dccaeea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Setting up:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(**TRAINER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBDT_CONFIG_LAG7   = {**BASE_CONFIG, \"lags\": list(range(-7, -1))}\n",
    "GBDT_CONFIG_LAG365 = {**BASE_CONFIG, \"lags\": list(range(-365, -1))}\n",
    "GBDT_CONFIG_LAG730 = {**BASE_CONFIG, \"lags\": list(range(-730, -1))}\n",
    "\n",
    "FINAL_MODELS = [\"lgbm\", \"lgbm\", \"lgbm\"]\n",
    "FINAL_CONFIGS = [GBDT_CONFIG_LAG7, GBDT_CONFIG_LAG365, GBDT_CONFIG_LAG730]\n",
    "\n",
    "# forecast_horizon = (pd.to_datetime(\"2025-08-16\") - pd.to_datetime(\"2017-08-16\")).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate forecasts for model trained on the entire data\n",
    "# predictions1 = trainer.ensemble_predict(\n",
    "#     model_names=FINAL_MODELS, \n",
    "#     model_configs=FINAL_CONFIGS,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menjalankan ensemble_predict untuk melatih dan menangkap model final...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d018d1418247cc8dde126a873efbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating forecasts:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.633610\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.658206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054367 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205301\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.681446\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19740\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.064203\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 37687\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.081576\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 50147\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.106834\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.476847\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112100\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.504133\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 204907\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.541758\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.858728\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026837 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.880745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.894449\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19794\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.019415\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33797\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 472\n",
      "[LightGBM] [Info] Start training from score 0.024669\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33783\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 472\n",
      "[LightGBM] [Info] Start training from score 0.034068\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008358 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.880245\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.890171\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.899076\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.372289\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.473030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205206\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.523854\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.879622\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.887997\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.895953\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.854156\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028832 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.881676\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.892010\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.864810\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.879892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.893958\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.798350\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025671 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.815601\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.822547\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.588347\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.605834\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.618253\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009327 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.860617\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.868473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065884 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.876894\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.563738\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.570367\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039376 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.584369\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007790 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20340\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.416174\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 72972\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.433379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 121531\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.438158\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009250 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.434143\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029869 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.551621\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046061 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.628445\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.455533\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.578799\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.680001\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19950\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.300381\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024757 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49462\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.295129\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039504 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 78086\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.293390\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006510 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.526366\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.668799\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.739220\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.318604\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033844 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.404817\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205134\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.435984\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.268183\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037909 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.297749\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.316373\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.576406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.572401\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.560226\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.572269\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.585616\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.603471\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.276411\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041404 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112173\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.351207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054877 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198957\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.444811\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.782244\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025938 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.787139\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.794218\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.803406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.817062\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.829229\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009827 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20957\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.341067\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112170\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.433359\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 204248\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.499959\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.364774\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.463481\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.521192\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.798982\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027015 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.816507\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.819658\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.829052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026781 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.844378\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.861148\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.578163\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.713640\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.774747\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.139762\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112246\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.177581\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 203048\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.207824\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010373 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20958\n",
      "[LightGBM] [Info] Number of data points in the train set: 8405, number of used features: 179\n",
      "[LightGBM] [Info] Start training from score 0.661176\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 112248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6615, number of used features: 537\n",
      "[LightGBM] [Info] Start training from score 0.679862\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 205309\n",
      "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 902\n",
      "[LightGBM] [Info] Start training from score 0.704577\n",
      "Melihat semua atribut dan metode yang tersedia untuk objek pertama di ens_pred:\n",
      "['__abs__', '__add__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__pow__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__round__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_assert_deterministic', '_assert_stochastic', '_assert_univariate', '_attrs', '_bottom_level_components', '_components', '_extract_values', '_fill_missing_dates', '_freq', '_freq_str', '_get_agg_dims', '_get_axis', '_get_first_timestamp_after', '_get_last_timestamp_before', '_has_datetime_index', '_observed_freq_datetime_index', '_observed_freq_integer_index', '_raise_if_not_within', '_repr_html_', '_restore_from_frequency', '_restore_range_indexed', '_slice_intersect_bounds', '_sort_index', '_split_at', '_time_dim', '_time_index', '_top_level_component', '_values', 'add_datetime_attribute', 'add_holidays', 'all_values', 'append', 'append_values', 'astype', 'bottom_level_components', 'bottom_level_series', 'columns', 'components', 'concatenate', 'copy', 'cumsum', 'data_array', 'diff', 'drop_after', 'drop_before', 'drop_columns', 'dtype', 'duration', 'end_time', 'first_value', 'first_values', 'freq', 'freq_str', 'from_csv', 'from_dataframe', 'from_group_dataframe', 'from_json', 'from_pickle', 'from_series', 'from_times_and_values', 'from_values', 'from_xarray', 'gaps', 'get_index_at_point', 'get_timestamp_at_point', 'has_datetime_index', 'has_hierarchy', 'has_metadata', 'has_range_index', 'has_same_time_as', 'has_static_covariates', 'head', 'hierarchy', 'is_deterministic', 'is_probabilistic', 'is_stochastic', 'is_univariate', 'is_within_range', 'kurtosis', 'last_value', 'last_values', 'longest_contiguous_slice', 'map', 'max', 'mean', 'median', 'metadata', 'min', 'n_components', 'n_samples', 'n_timesteps', 'plot', 'prepend', 'prepend_values', 'quantile', 'random_component_values', 'resample', 'rescale_with_value', 'schema', 'shape', 'shift', 'skew', 'slice', 'slice_intersect', 'slice_intersect_times', 'slice_intersect_values', 'slice_n_points_after', 'slice_n_points_before', 'split_after', 'split_before', 'stack', 'start_time', 'static_covariates', 'static_covariates_values', 'std', 'strip', 'sum', 'tail', 'time_dim', 'time_index', 'to_csv', 'to_dataframe', 'to_json', 'to_pickle', 'to_series', 'top_level_component', 'top_level_series', 'univariate_component', 'univariate_values', 'values', 'var', 'width', 'window_transform', 'with_columns_renamed', 'with_hierarchy', 'with_metadata', 'with_static_covariates', 'with_times_and_values', 'with_values']\n",
      "Proses training dan prediksi selesai. Model final sudah tersimpan di dalam objek trainer.\n",
      "✅ Objek 'trainer' berhasil disimpan.\n",
      "✅ Kamus model final ('final_models_dict.pkl') berhasil disimpan.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# KODE UNTUK DIJALANKAN DI NOTEBOOK ANDA SETELAH CLASS TRAINER DIPERBARUI\n",
    "# =============================================================================\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "print(\"Menjalankan ensemble_predict untuk melatih dan menangkap model final...\")\n",
    "predictions1 = trainer.ensemble_predict(\n",
    "    model_names=FINAL_MODELS, \n",
    "    model_configs=FINAL_CONFIGS,\n",
    ")\n",
    "print(\"Proses training dan prediksi selesai. Model final sudah tersimpan di dalam objek trainer.\")\n",
    "\n",
    "ARTIFACTS_DIR = \"artifacts_nadi_pasar\"\n",
    "if not os.path.exists(ARTIFACTS_DIR):\n",
    "    os.makedirs(ARTIFACTS_DIR)\n",
    "\n",
    "with open(os.path.join(ARTIFACTS_DIR, 'trainer_object.pkl'), 'wb') as f:\n",
    "    pickle.dump(trainer, f)\n",
    "print(f\"✅ Objek 'trainer' berhasil disimpan.\")\n",
    "\n",
    "with open(os.path.join(ARTIFACTS_DIR, 'final_models_dict.pkl'), 'wb') as f:\n",
    "    pickle.dump(trainer.last_fitted_models_, f)\n",
    "print(f\"✅ Kamus model final ('final_models_dict.pkl') berhasil disimpan.\")\n",
    "\n",
    "# 3. (Jangan lupa) Simpan pemetaan produk ke family Anda\n",
    "# product_to_family_map = {...}\n",
    "# with open(os.path.join(ARTIFACTS_DIR, 'product_to_family_map.pkl'), 'wb') as f:\n",
    "#     pickle.dump(product_to_family_map, f)\n",
    "# print(f\"Pemetaan produk ('product_to_family_map.pkl') berhasil disimpan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>oil</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>date_index</th>\n",
       "      <th>type_Additional</th>\n",
       "      <th>type_Holiday</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>type_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>type_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>type_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>type_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>type_E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760810</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>type_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760811</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>type_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760812</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>type_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760813</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>type_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760814</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>type_E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>760815 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date      family  sales    oil  day  month  year  day_of_week  \\\n",
       "0      2013-01-01  AUTOMOTIVE    0.0  93.12    1      1  2013            1   \n",
       "1      2013-01-01  AUTOMOTIVE    0.0  93.12    1      1  2013            1   \n",
       "2      2013-01-01  AUTOMOTIVE    0.0  93.12    1      1  2013            1   \n",
       "3      2013-01-01  AUTOMOTIVE    0.0  93.12    1      1  2013            1   \n",
       "4      2013-01-01  AUTOMOTIVE    0.0  93.12    1      1  2013            1   \n",
       "...           ...         ...    ...    ...  ...    ...   ...          ...   \n",
       "760810 2025-08-16     SEAFOOD    0.0  62.80   16      8  2025            5   \n",
       "760811 2025-08-16     SEAFOOD    0.0  62.80   16      8  2025            5   \n",
       "760812 2025-08-16     SEAFOOD    0.0  62.80   16      8  2025            5   \n",
       "760813 2025-08-16     SEAFOOD    0.0  62.80   16      8  2025            5   \n",
       "760814 2025-08-16     SEAFOOD    0.0  62.80   16      8  2025            5   \n",
       "\n",
       "        day_of_year  week_of_year  date_index  type_Additional  type_Holiday  \\\n",
       "0                 1             1           0              0.0           1.0   \n",
       "1                 1             1           0              0.0           1.0   \n",
       "2                 1             1           0              0.0           1.0   \n",
       "3                 1             1           0              0.0           1.0   \n",
       "4                 1             1           0              0.0           1.0   \n",
       "...             ...           ...         ...              ...           ...   \n",
       "760810          228            33        4610              0.0           0.0   \n",
       "760811          228            33        4610              0.0           0.0   \n",
       "760812          228            33        4610              0.0           0.0   \n",
       "760813          228            33        4610              0.0           0.0   \n",
       "760814          228            33        4610              0.0           0.0   \n",
       "\n",
       "          type  \n",
       "0       type_A  \n",
       "1       type_B  \n",
       "2       type_C  \n",
       "3       type_D  \n",
       "4       type_E  \n",
       "...        ...  \n",
       "760810  type_A  \n",
       "760811  type_B  \n",
       "760812  type_C  \n",
       "760813  type_D  \n",
       "760814  type_E  \n",
       "\n",
       "[760815 rows x 14 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "      <th>type</th>\n",
       "      <th>family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>10.085077</td>\n",
       "      <td>type_A</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>10.246009</td>\n",
       "      <td>type_A</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>11.022766</td>\n",
       "      <td>type_A</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>16.914660</td>\n",
       "      <td>type_A</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>18.052866</td>\n",
       "      <td>type_A</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479980</th>\n",
       "      <td>2025-07-29</td>\n",
       "      <td>4.170287</td>\n",
       "      <td>type_E</td>\n",
       "      <td>SEAFOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479981</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>3.947154</td>\n",
       "      <td>type_E</td>\n",
       "      <td>SEAFOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479982</th>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>4.257635</td>\n",
       "      <td>type_E</td>\n",
       "      <td>SEAFOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479983</th>\n",
       "      <td>2025-08-01</td>\n",
       "      <td>4.655717</td>\n",
       "      <td>type_E</td>\n",
       "      <td>SEAFOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479984</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>6.170019</td>\n",
       "      <td>type_E</td>\n",
       "      <td>SEAFOOD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479985 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date      sales    type      family\n",
       "0      2017-08-16  10.085077  type_A  AUTOMOTIVE\n",
       "1      2017-08-17  10.246009  type_A  AUTOMOTIVE\n",
       "2      2017-08-18  11.022766  type_A  AUTOMOTIVE\n",
       "3      2017-08-19  16.914660  type_A  AUTOMOTIVE\n",
       "4      2017-08-20  18.052866  type_A  AUTOMOTIVE\n",
       "...           ...        ...     ...         ...\n",
       "479980 2025-07-29   4.170287  type_E     SEAFOOD\n",
       "479981 2025-07-30   3.947154  type_E     SEAFOOD\n",
       "479982 2025-07-31   4.257635  type_E     SEAFOOD\n",
       "479983 2025-08-01   4.655717  type_E     SEAFOOD\n",
       "479984 2025-08-02   6.170019  type_E     SEAFOOD\n",
       "\n",
       "[479985 rows x 4 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>oil</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>date_index</th>\n",
       "      <th>type_Additional</th>\n",
       "      <th>type_Holiday</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>type_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>type_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>type_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>type_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>type_E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2680750</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>type_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2680751</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>type_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2680752</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>type_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2680753</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>type_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2680754</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.80</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>228</td>\n",
       "      <td>33</td>\n",
       "      <td>4610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>type_E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2680755 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date      family  sales    oil  day  month  year  day_of_week  \\\n",
       "0       2013-01-01  AUTOMOTIVE    0.0  93.12    1      1  2013            1   \n",
       "1       2013-01-01  AUTOMOTIVE    0.0  93.12    1      1  2013            1   \n",
       "2       2013-01-01  AUTOMOTIVE    0.0  93.12    1      1  2013            1   \n",
       "3       2013-01-01  AUTOMOTIVE    0.0  93.12    1      1  2013            1   \n",
       "4       2013-01-01  AUTOMOTIVE    0.0  93.12    1      1  2013            1   \n",
       "...            ...         ...    ...    ...  ...    ...   ...          ...   \n",
       "2680750 2025-08-16     SEAFOOD    0.0  62.80   16      8  2025            5   \n",
       "2680751 2025-08-16     SEAFOOD    0.0  62.80   16      8  2025            5   \n",
       "2680752 2025-08-16     SEAFOOD    0.0  62.80   16      8  2025            5   \n",
       "2680753 2025-08-16     SEAFOOD    0.0  62.80   16      8  2025            5   \n",
       "2680754 2025-08-16     SEAFOOD    0.0  62.80   16      8  2025            5   \n",
       "\n",
       "         day_of_year  week_of_year  date_index  type_Additional  type_Holiday  \\\n",
       "0                  1             1           0              0.0           1.0   \n",
       "1                  1             1           0              0.0           1.0   \n",
       "2                  1             1           0              0.0           1.0   \n",
       "3                  1             1           0              0.0           1.0   \n",
       "4                  1             1           0              0.0           1.0   \n",
       "...              ...           ...         ...              ...           ...   \n",
       "2680750          228            33        4610              0.0           0.0   \n",
       "2680751          228            33        4610              0.0           0.0   \n",
       "2680752          228            33        4610              0.0           0.0   \n",
       "2680753          228            33        4610              0.0           0.0   \n",
       "2680754          228            33        4610              0.0           0.0   \n",
       "\n",
       "           type  \n",
       "0        type_A  \n",
       "1        type_B  \n",
       "2        type_C  \n",
       "3        type_D  \n",
       "4        type_E  \n",
       "...         ...  \n",
       "2680750  type_A  \n",
       "2680751  type_B  \n",
       "2680752  type_C  \n",
       "2680753  type_D  \n",
       "2680754  type_E  \n",
       "\n",
       "[2680755 rows x 14 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.merge(\n",
    "    predictions1[['date','family','sales']],\n",
    "    on=['date','family'],\n",
    "    how='left',\n",
    "    suffixes=('', '_pred')\n",
    ")\n",
    "\n",
    "# Hanya replace sales jika ada prediksi, biarkan yang lain tetap\n",
    "data['sales'] = data['sales_pred'].combine_first(data['sales'])\n",
    "data = data.drop(columns=['sales_pred'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2025-08-03'\n",
    "end_date = '2025-08-16'\n",
    "\n",
    "data = data[~((data['date'] >= start_date) & (data['date'] <= end_date))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>oil</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>date_index</th>\n",
       "      <th>type_Additional</th>\n",
       "      <th>type_Holiday</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.120000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>type_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.120000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>type_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.120000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>type_C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.120000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>type_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.120000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>type_E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2678440</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>52.562431</td>\n",
       "      <td>66.983333</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>214</td>\n",
       "      <td>31</td>\n",
       "      <td>4596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>type_E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2678441</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>15.001511</td>\n",
       "      <td>66.983333</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>214</td>\n",
       "      <td>31</td>\n",
       "      <td>4596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>type_E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2678442</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>7.053787</td>\n",
       "      <td>66.983333</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>214</td>\n",
       "      <td>31</td>\n",
       "      <td>4596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>type_E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2678443</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>29.794912</td>\n",
       "      <td>66.983333</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>214</td>\n",
       "      <td>31</td>\n",
       "      <td>4596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>type_E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2678444</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>6.170019</td>\n",
       "      <td>66.983333</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>214</td>\n",
       "      <td>31</td>\n",
       "      <td>4596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>type_E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2678445 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date      family      sales        oil  day  month  year  \\\n",
       "0       2013-01-01  AUTOMOTIVE   0.000000  93.120000    1      1  2013   \n",
       "1       2013-01-01  AUTOMOTIVE   0.000000  93.120000    1      1  2013   \n",
       "2       2013-01-01  AUTOMOTIVE   0.000000  93.120000    1      1  2013   \n",
       "3       2013-01-01  AUTOMOTIVE   0.000000  93.120000    1      1  2013   \n",
       "4       2013-01-01  AUTOMOTIVE   0.000000  93.120000    1      1  2013   \n",
       "...            ...         ...        ...        ...  ...    ...   ...   \n",
       "2678440 2025-08-02     SEAFOOD  52.562431  66.983333    2      8  2025   \n",
       "2678441 2025-08-02     SEAFOOD  15.001511  66.983333    2      8  2025   \n",
       "2678442 2025-08-02     SEAFOOD   7.053787  66.983333    2      8  2025   \n",
       "2678443 2025-08-02     SEAFOOD  29.794912  66.983333    2      8  2025   \n",
       "2678444 2025-08-02     SEAFOOD   6.170019  66.983333    2      8  2025   \n",
       "\n",
       "         day_of_week  day_of_year  week_of_year  date_index  type_Additional  \\\n",
       "0                  1            1             1           0              0.0   \n",
       "1                  1            1             1           0              0.0   \n",
       "2                  1            1             1           0              0.0   \n",
       "3                  1            1             1           0              0.0   \n",
       "4                  1            1             1           0              0.0   \n",
       "...              ...          ...           ...         ...              ...   \n",
       "2678440            5          214            31        4596              0.0   \n",
       "2678441            5          214            31        4596              0.0   \n",
       "2678442            5          214            31        4596              0.0   \n",
       "2678443            5          214            31        4596              0.0   \n",
       "2678444            5          214            31        4596              0.0   \n",
       "\n",
       "         type_Holiday    type  \n",
       "0                 1.0  type_A  \n",
       "1                 1.0  type_B  \n",
       "2                 1.0  type_C  \n",
       "3                 1.0  type_D  \n",
       "4                 1.0  type_E  \n",
       "...               ...     ...  \n",
       "2678440           0.0  type_E  \n",
       "2678441           0.0  type_E  \n",
       "2678442           0.0  type_E  \n",
       "2678443           0.0  type_E  \n",
       "2678444           0.0  type_E  \n",
       "\n",
       "[2678445 rows x 14 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>family</th>\n",
       "      <th>type</th>\n",
       "      <th>sales</th>\n",
       "      <th>oil</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>date_index</th>\n",
       "      <th>type_Additional</th>\n",
       "      <th>type_Holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_A</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.120000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_B</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.120000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_C</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.120000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_D</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.120000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>type_E</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.120000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758500</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_A</td>\n",
       "      <td>22.116532</td>\n",
       "      <td>66.983333</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>214</td>\n",
       "      <td>31</td>\n",
       "      <td>4596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758501</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_B</td>\n",
       "      <td>22.116532</td>\n",
       "      <td>66.983333</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>214</td>\n",
       "      <td>31</td>\n",
       "      <td>4596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758502</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_C</td>\n",
       "      <td>22.116532</td>\n",
       "      <td>66.983333</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>214</td>\n",
       "      <td>31</td>\n",
       "      <td>4596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758503</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_D</td>\n",
       "      <td>22.116532</td>\n",
       "      <td>66.983333</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>214</td>\n",
       "      <td>31</td>\n",
       "      <td>4596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758504</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>type_E</td>\n",
       "      <td>22.116532</td>\n",
       "      <td>66.983333</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>214</td>\n",
       "      <td>31</td>\n",
       "      <td>4596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>758505 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date      family    type      sales        oil  day  month  year  \\\n",
       "0      2013-01-01  AUTOMOTIVE  type_A   0.000000  93.120000    1      1  2013   \n",
       "1      2013-01-01  AUTOMOTIVE  type_B   0.000000  93.120000    1      1  2013   \n",
       "2      2013-01-01  AUTOMOTIVE  type_C   0.000000  93.120000    1      1  2013   \n",
       "3      2013-01-01  AUTOMOTIVE  type_D   0.000000  93.120000    1      1  2013   \n",
       "4      2013-01-01  AUTOMOTIVE  type_E   0.000000  93.120000    1      1  2013   \n",
       "...           ...         ...     ...        ...        ...  ...    ...   ...   \n",
       "758500 2025-08-02     SEAFOOD  type_A  22.116532  66.983333    2      8  2025   \n",
       "758501 2025-08-02     SEAFOOD  type_B  22.116532  66.983333    2      8  2025   \n",
       "758502 2025-08-02     SEAFOOD  type_C  22.116532  66.983333    2      8  2025   \n",
       "758503 2025-08-02     SEAFOOD  type_D  22.116532  66.983333    2      8  2025   \n",
       "758504 2025-08-02     SEAFOOD  type_E  22.116532  66.983333    2      8  2025   \n",
       "\n",
       "        day_of_week  day_of_year  week_of_year  date_index  type_Additional  \\\n",
       "0                 1            1             1           0              0.0   \n",
       "1                 1            1             1           0              0.0   \n",
       "2                 1            1             1           0              0.0   \n",
       "3                 1            1             1           0              0.0   \n",
       "4                 1            1             1           0              0.0   \n",
       "...             ...          ...           ...         ...              ...   \n",
       "758500            5          214            31        4596              0.0   \n",
       "758501            5          214            31        4596              0.0   \n",
       "758502            5          214            31        4596              0.0   \n",
       "758503            5          214            31        4596              0.0   \n",
       "758504            5          214            31        4596              0.0   \n",
       "\n",
       "        type_Holiday  \n",
       "0                1.0  \n",
       "1                1.0  \n",
       "2                1.0  \n",
       "3                1.0  \n",
       "4                1.0  \n",
       "...              ...  \n",
       "758500           0.0  \n",
       "758501           0.0  \n",
       "758502           0.0  \n",
       "758503           0.0  \n",
       "758504           0.0  \n",
       "\n",
       "[758505 rows x 14 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# buat aturan agregasi\n",
    "agg_rules = {\n",
    "    \"sales\": \"mean\",   # rata-rata untuk sales\n",
    "    \"oil\": \"first\",    # misalnya ambil nilai pertama (karena biasanya sama per tanggal)\n",
    "    \"day\": \"first\",\n",
    "    \"month\": \"first\",\n",
    "    \"year\": \"first\",\n",
    "    \"day_of_week\": \"first\",\n",
    "    \"day_of_year\": \"first\",\n",
    "    \"week_of_year\": \"first\",\n",
    "    \"date_index\": \"first\",\n",
    "    \"type_Additional\": \"first\",\n",
    "    \"type_Holiday\": \"first\"\n",
    "}\n",
    "\n",
    "# lakukan groupby dan simpan ke data baru\n",
    "data = (\n",
    "    data.groupby([\"date\", \"family\", \"type\"], as_index=False)\n",
    "        .agg(agg_rules)\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('dataset_fix.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 2887556,
     "sourceId": 29781,
     "sourceType": "competition"
    },
    {
     "datasetId": 8078397,
     "sourceId": 12778208,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
